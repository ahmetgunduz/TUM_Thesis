@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{hara3dcnns,
  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},
  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6546--6555},
  year={2018},
}

@article{kopuklu2018motion,
  title={Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition},
  author={K{\"o}p{\"u}kl{\"u}, Okan and K{\"o}se, Neslihan and Rigoll, Gerhard},
  journal={arXiv preprint arXiv:1804.07187},
  year={2018}
}

@article{ohn2014hand,
  title={Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations},
  author={Ohn-Bar, Eshed and Trivedi, Mohan Manubhai},
  journal={IEEE transactions on intelligent transportation systems},
  volume={15},
  number={6},
  pages={2368--2377},
  year={2014},
  publisher={IEEE}
}

@inproceedings{molchanov2015multi,
  title={Multi-sensor system for driver's hand-gesture recognition},
  author={Molchanov, Pavlo and Gupta, Shalini and Kim, Kihwan and Pulli, Kari},
  booktitle={Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on},
  volume={1},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

@article{Gavrila1999TheVA,
  title={The Visual Analysis of Human Movement: A Survey},
  author={Dariu Gavrila},
  journal={Computer Vision and Image Understanding},
  year={1999},
  volume={73},
  pages={82-98}
}

@inproceedings{molchanov2016online,
	title={Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network},
	author={Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4207--4215},
	year={2016}
}

@article{ohn2014hand,
	title={Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations},
	author={Ohn-Bar, Eshed and Trivedi, Mohan Manubhai},
	journal={IEEE transactions on intelligent transportation systems},
	volume={15},
	number={6},
	pages={2368--2377},
	year={2014},
	publisher={IEEE}
}

@inproceedings{simonyan2014two,
	title={Two-stream convolutional networks for action recognition in videos},
	author={Simonyan, Karen and Zisserman, Andrew},
	booktitle={Advances in neural information processing systems},
	pages={568--576},
	year={2014}
}

@article{wang2016robust,
	title={A robust and efficient video representation for action recognition},
	author={Wang, Heng and Oneata, Dan and Verbeek, Jakob and Schmid, Cordelia},
	journal={International Journal of Computer Vision},
	volume={119},
	number={3},
	pages={219--238},
	year={2016},
	publisher={Springer}
}

@inproceedings{tran2015learning,
	title={Learning spatiotemporal features with 3d convolutional networks},
	author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	booktitle={Computer Vision (ICCV), 2015 IEEE International Conference on},
	pages={4489--4497},
	year={2015},
	organization={IEEE}
}

@article{varol2017long,
	title={Long-term temporal convolutions for action recognition},
	author={Varol, Gul and Laptev, Ivan and Schmid, Cordelia},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	year={2017},
	publisher={IEEE}
}

@inproceedings{brox2004high,
	title={High accuracy optical flow estimation based on a theory for warping},
	author={Brox, Thomas and Bruhn, Andr{\'e}s and Papenberg, Nils and Weickert, Joachim},
	booktitle={European conference on computer vision},
	pages={25--36},
	year={2004},
	organization={Springer}
}

@inproceedings{farneback2003two,
	title={Two-frame motion estimation based on polynomial expansion},
	author={Farneb{\"a}ck, Gunnar},
	booktitle={Scandinavian conference on Image analysis},
	pages={363--370},
	year={2003},
	organization={Springer}
}

@inproceedings{kantorov2014efficient,
	title={Efficient feature extraction, encoding and classification for action recognition},
	author={Kantorov, Vadim and Laptev, Ivan},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={2593--2600},
	year={2014}
}

@misc{jester,
	howpublished = {\url{https://www.twentybn.com/datasets/jester/v1}},
}

@inproceedings{wang2016temporal,
	title={Temporal segment networks: Towards good practices for deep action recognition},
	author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
	booktitle={European Conference on Computer Vision},
	pages={20--36},
	year={2016},
	organization={Springer}
}

@inproceedings{ioffe2015batch,
	title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	author={Ioffe, Sergey and Szegedy, Christian},
	booktitle={International conference on machine learning},
	pages={448--456},
	year={2015}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@inproceedings{wan2016chalearn,
	title={Chalearn looking at people rgb-d isolated and continuous datasets for gesture recognition},
	author={Wan, Jun and Zhao, Yibing and Zhou, Shuai and Guyon, Isabelle and Escalera, Sergio and Li, Stan Z},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
	pages={56--64},
	year={2016}
}

@article{zhou2017temporal,
	title={Temporal Relational Reasoning in Videos},
	author={Zhou, Bolei and Andonian, Alex and Torralba, Antonio},
	journal={arXiv preprint arXiv:1711.08496},
	year={2017}
}

@article{paszke2017automatic,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year={2017}
}

@article{zhu2017multimodal,
	title={Multimodal gesture recognition using 3-D convolution and convolutional LSTM},
	author={Zhu, Guangming and Zhang, Liang and Shen, Peiyi and Song, Juan},
	journal={IEEE Access},
	volume={5},
	pages={4517--4524},
	year={2017},
	publisher={IEEE}
}

@INPROCEEDINGS{7899602, 
	author={Yunan Li and Qiguang Miao and Kuan Tian and Yingying Fan and Xin Xu and Rui Li and J. Song}, 
	booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
	title={Large-scale gesture recognition with a fusion of RGB-D data based on the C3D model}, 
	year={2016}, 
	volume={}, 
	number={}, 
	pages={25-30}, 
	keywords={gesture recognition;image classification;sensor fusion;support vector machines;C3D model;Chalearn LAP IsoGD Database;RGB-D data fusion;computer vision;large-scale gesture recognition;linear SVM classifier;Databases;Feature extraction;Gesture recognition;Hidden Markov models;Solid modeling;Three-dimensional displays;Videos}, 
	doi={10.1109/ICPR.2016.7899602}, 
	ISSN={}, 
	month={Dec},}

@article{duan2017unified,
	title={A unified framework for multi-modal isolated gesture recognition},
	author={Duan, Jiali and Wan, Jun and Zhou, Shuai and Guo, Xiaoyuan and Li, S and others},
	journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),(Accept)},
	volume={3},
	year={2017}
}

@inproceedings{zhu2016large,
	title={Large-scale isolated gesture recognition using pyramidal 3d convolutional networks},
	author={Zhu, Guangming and Zhang, Liang and Mei, Lin and Shao, Jie and Song, Juan and Shen, Peiyi},
	booktitle={Pattern Recognition (ICPR), 2016 23rd International Conference on},
	pages={19--24},
	year={2016},
	organization={IEEE}
}

@inproceedings{wang2016large,
	title={Large-scale isolated gesture recognition using convolutional neural networks},
	author={Wang, Pichao and Li, Wanqing and Liu, Song and Gao, Zhimin and Tang, Chang and Ogunbona, Philip},
	booktitle={Pattern Recognition (ICPR), 2016 23rd International Conference on},
	pages={7--12},
	year={2016},
	organization={IEEE}
}

@inproceedings{miao2017multimodal,
	title={Multimodal Gesture Recognition Based on the ResC3D Network},
	author={Miao, Qiguang and Li, Yunan and Ouyang, Wanli and Ma, Zhenxin and Xu, Xin and Shi, Weikang and Cao, Xiaochun and Liu, Zhipeng and Chai, Xiujuan and Liu, Zhuang and others},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={3047--3055},
	year={2017}
}

@inproceedings{wang2017large,
	title={Large-scale Multimodal Gesture Recognition Using Heterogeneous Networks},
	author={Wang, Huogen and Wang, Pichao and Song, Zhanjie and Li, Wanqing},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={3129--3137},
	year={2017}
}

@inproceedings{wang2017scene,
	title={Scene flow to action map: A new representation for rgb-d based action recognition with convolutional neural networks},
	author={Wang, Pichao and Li, Wanqing and Gao, Zhimin and Zhang, Yuyao and Tang, Chang and Ogunbona, Philip},
	booktitle={Proc. Comput. Vis. Pattern Recognit.},
	pages={1--10},
	year={2017}
}

@inproceedings{zhang2017learning,
	title={Learning Spatiotemporal Features Using 3DCNN and Convolutional LSTM for Gesture Recognition},
	author={Zhang, Liang and Zhu, Guangming and Shen, Peiyi and Song, Juan and Shah, Syed Afaq and Bennamoun, Mohammed},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={3120--3128},
	year={2017}
}


@inproceedings{simonyan2014two,
	title={Two-stream convolutional networks for action recognition in videos},
	author={Simonyan, Karen and Zisserman, Andrew},
	booktitle={Advances in neural information processing systems},
	pages={568--576},
	year={2014}
}

@inproceedings{Feichtenhofer2016convolutional,
	title = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
	author= {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	booktitle={Computer Vision and Pattern Recognition (CVPR)},
	year={2016}
}

@inproceedings{liu20163dbased,
	title={3d-based deep convolutional neural network for action recognition with depth sequences},
	author={Liu, Zhi and Zhang, Chenyang and Yingli, Tian},
	journal={Image and Vision Computing},
	volume={55},
	pages={93--100},
	year={2016}
}


@inproceedings{asadi2017survey,
	title={A Survey on Deep Learning Based Approaches for Action and Gesture Recognition in Image Sequences},
	author={Asadi-Aghbolaghi, M. and Clapés, A. and Bellantonio, M. and Escalante, H. J. and Ponce-López, V. and Baró, X. and Guyon, I. and Kasaei, S. and Escalera, S.},
	booktitle={2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
	pages={476-483},
	year={2017}
}

@article{elman1990finding,
	author = {Elman, Jeffrey L.},
	title = {Finding structure in time},
	journal = {COGNITIVE SCIENCE},
	year = {1990},
	volume = {14},
	number = {2},
	pages = {179--211}
}

@article{gers2002learning,
	author={Gers, Felix A. and Schraudolph, Nicol N. and Schmidhuber, Jrgen}, 
	Title={Learning Precise Timing with LSTM Recurrent Networks},
	journal={Journal of Machine Learning Research},
	volume={3}, 
	year = {2002},
	pages = {115--143}
}

@inproceedings{du2015hierarchical,
	title={Hierarchical recurrent neural network for skeleton based action recognition},
	author={Du, Yong and Wang, W. and Wang, L.},
	booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2015},
	pages={1110-1118}
}


@inproceedings{zhou2014learning,
	title={Learning deep features for scene recognition using places database},
	author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and OLiva, Aude},
	booktitle={Advances in Neural Information Processing Systems 27 (NIPS)},
	pages={487--495},
	year={2014}
}

@inproceedings{Girshick2014rich,
	title= {Rich feature hierarchies for accurate object detection and semantic segmentation},
	author= {Girshick, Ross B. and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	booktitle={Computer Vision and Pattern Recognition (CVPR)},
	pages={580--587},
	year={2014}
	
}

@inproceedings{Feichtenhofer2016spatiotemporal,
	title = {Spatiotemporal Residual Networks for Video Action Recognition},
	author= {Feichtenhofer, Christoph and Pinz, Axel and Wildes, Richard P.},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	pages={3468--3476},
	year={2016}
}

@inproceedings{Feichtenhofer2016convolutional,
	title = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
	author= {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	booktitle={Computer Vision and Pattern Recognition (CVPR)},
	year={2016}
}


@inproceedings{Karpathy2014largescale,
	title={Large-Scale Video Classification with Convolutional Neural Networks},
	author={Karpathy, A. and Toderici, G. and Shetty, S. and Leung, T. and Sukthankar R. and Fei-Fei, L.},
	booktitle={Computer Vision and Pattern Recognition (CVPR)},
	pages={1725-1732},
	year={2014}
}

@inproceedings{Wang2015towards,
	title={Towards good practices for very deep twostream convnets},
	author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu},
	journal  = {CoRR},
	volume = {abs/1507.02159},
	year = {2015},
	url = {http://arxiv.org/abs/1507.02159},
}

@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}

@article{tran2017convnet,
	title={Convnet architecture search for spatiotemporal feature learning},
	author={Tran, Du and Ray, Jamie and Shou, Zheng and Chang, Shih-Fu and Paluri, Manohar},
	journal={arXiv preprint arXiv:1708.05038},
	year={2017}
}

@inproceedings{donahue2015long,
	title={Long-term recurrent convolutional networks for visual recognition and description},
	author={Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2625--2634},
	year={2015}
}

@inproceedings{simard2003best,
	title={Best practices for convolutional neural networks applied to visual document analysis.},
	author={Simard, Patrice Y and Steinkraus, David and Platt, John C and others},
	booktitle={ICDAR},
	volume={3},
	pages={958--962},
	year={2003}
}



@inproceedings{perronnin_improving_2010,
	title = {Improving the {Fisher} {Kernel} for {Large}-{Scale} {Image} {Classification}},
	volume = {6314},
	doi = {10.1007/978-3-642-15561-1_11},
	abstract = {The Fisher kernel (FK) is a generic framework which com- bines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9\% to 58.3\%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained us- ing only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant re- sources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.},
	booktitle = {Proceedings of {IEEE} {European} {Conference} on {Computer} {Vision}, 2010},
	author = {Perronnin, Florent and Sánchez, Jorge and Mensink, Thomas},
	month = sep,
	year = {2010},
	pages = {143--156}
}

@article{wang_robust_2015,
	title = {A robust and efficient video representation for action recognition},
	url = {http://arxiv.org/abs/1504.05524},
	abstract = {This paper introduces a state-of-the-art video representation and applies it to efficient action recognition and detection. We first propose to improve the popular dense trajectory features by explicit camera motion estimation. More specifically, we extract feature point matches between frames using SURF descriptors and dense optical flow. The matches are used to estimate a homography with RANSAC. To improve the robustness of homography estimation, a human detector is employed to remove outlier matches from the human body as human motion is not constrained by the camera. Trajectories consistent with the homography are considered as due to camera motion, and thus removed. We also use the homography to cancel out camera motion from the optical flow. This results in significant improvement on motion-based HOF and MBH descriptors. We further explore the recent Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram, and consider different ways to include spatial layout information in these encodings. We present a large and varied set of evaluations, considering (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that our improved trajectory features significantly outperform previous dense trajectories, and that Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, we show substantial improvements over the state-of-the-art results.},
	urldate = {2018-09-11TZ},
	journal = {arXiv:1504.05524 [cs]},
	author = {Wang, Heng and Oneata, Dan and Verbeek, Jakob and Schmid, Cordelia},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.05524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{tamrakar_evaluation_2012,
	title = {Evaluation of low-level features and their combinations for complex event detection in open source videos},
	doi = {10.1109/CVPR.2012.6248114},
	abstract = {Low-level appearance as well as spatio-temporal features, appropriately quantized and aggregated into Bag-of-Words (BoW) descriptors, have been shown to be effective in many detection and recognition tasks. However, their effcacy for complex event recognition in unconstrained videos have not been systematically evaluated. In this paper, we use the NIST TRECVID Multimedia Event Detection (MED11 [1]) open source dataset, containing annotated data for 15 high-level events, as the standardized test bed for evaluating the low-level features. This dataset contains a large number of user-generated video clips. We consider 7 different low-level features, both static and dynamic, using BoW descriptors within an SVM approach for event detection. We present performance results on the 15 MED11 events for each of the features as well as their combinations using a number of early and late fusion strategies and discuss their strengths and limitations.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Tamrakar, A. and Ali, S. and Yu, Q. and Liu, J. and Javed, O. and Divakaran, A. and Cheng, H. and Sawhney, H.},
	month = jun,
	year = {2012},
	keywords = {BoW descriptors, Computer vision, Event detection, Feature extraction, MED11 open source dataset, NIST TRECVID multimedia event detection, SVM approach, Support vector machines, Training, Trajectory, Videos, annotated data, bag-of-words descriptors, complex event detection, complex event recognition, detection task, early fusion strategy, feature extraction, image fusion, late fusion strategy, low-level appearance, low-level features evaluation, multimedia computing, object detection, object recognition, open source videos, public domain software, recognition task, spatio-temporal features, standardized test bed, support vector machines, unconstrained videos, user-generated video clips, video signal processing},
	pages = {3681--3688}
}

@article{shen_dynamic_2012,
	series = {Best of {Automatic} {Face} and {Gesture} {Recognition} 2011},
	title = {Dynamic hand gesture recognition: {An} exemplar-based approach from motion divergence fields},
	volume = {30},
	issn = {0262-8856},
	shorttitle = {Dynamic hand gesture recognition},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885611001193},
	doi = {10.1016/j.imavis.2011.11.003},
	abstract = {Exemplar-based approaches for dynamic hand gesture recognition usually require a large collection of gestures to achieve high-quality performance. Efficient visual representation of the motion patterns hence is very important to offer a scalable solution for gesture recognition when the databases are large. In this paper, we propose a new visual representation for hand motions based on the motion divergence fields, which can be normalized to gray-scale images. Salient regions such as Maximum Stable Extremal Regions (MSER) are then detected on the motion divergence maps. From each detected region, a local descriptor is extracted to capture local motion patterns. We further leverage indexing techniques from image search into gesture recognition. The extracted descriptors are indexed using a pre-trained vocabulary. A new gesture sample accordingly can be efficiently matched with database gestures through a term frequency-inverse document frequency (TF-IDF) weighting scheme. We have collected a hand gesture database with 10 categories and 1050 video samples for performance evaluation and further applications. The proposed method achieves higher recognition accuracy than other state-of-the-art motion and spatio-temporal features on this database. Besides, the average recognition time of our method for each gesture sequence is only 34.53ms.},
	number = {3},
	urldate = {2018-09-11TZ},
	journal = {Image and Vision Computing},
	author = {Shen, Xiaohui and Hua, Gang and Williams, Lance and Wu, Ying},
	month = mar,
	year = {2012},
	keywords = {Divergence fields, Hand gesture recognition, Maximum Stable Extremal Regions, Optical flow, Term frequency-inverse document frequency (TF-IDF)},
	pages = {227--235}
}

@inproceedings{molchanov_online_2016,
	address = {Las Vegas, NV, USA},
	title = {Online {Detection} and {Classification} of {Dynamic} {Hand} {Gestures} with {Recurrent} 3D {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780825/},
	doi = {10.1109/CVPR.2016.456},
	abstract = {Automatic detection and classiﬁcation of dynamic hand gestures in real-world systems intended for human computer interaction is challenging as: 1) there is a large diversity in how people perform gestures, making detection and classiﬁcation difﬁcult; 2) the system must work online in order to avoid noticeable lag between performing a gesture and its classiﬁcation; in fact, a negative lag (classiﬁcation before the gesture is ﬁnished) is desirable, as feedback to the user can then be truly instantaneous. In this paper, we address these challenges with a recurrent three-dimensional convolutional neural network that performs simultaneous detection and classiﬁcation of dynamic hand gestures from multi-modal data. We employ connectionist temporal classiﬁcation to train the network to predict class labels from inprogress gestures in unsegmented input streams. In order to validate our method, we introduce a new challenging multimodal dynamic hand gesture dataset captured with depth, color and stereo-IR sensors. On this challenging dataset, our gesture recognition system achieves an accuracy of 83.8\%, outperforms competing state-of-the-art algorithms, and approaches human accuracy of 88.4\%. Moreover, our method achieves state-of-the-art performance on SKIG and ChaLearn2014 benchmarks.},
	language = {en},
	urldate = {2018-08-25TZ},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
	month = jun,
	year = {2016},
	pages = {4207--4215}
}

@article{tran_learning_2014,
	title = {Learning {Spatiotemporal} {Features} with 3D {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1412.0767},
	abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1412.0767 [cs]},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.0767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://arxiv.org/abs/1406.2199},
	abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1406.2199 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{ji_3d_2010,
	title = {3D {Convolutional} {Neural} {Networks} for {Human} {Action} {Recognition}},
	url = {/paper/3D-Convolutional-Neural-Networks-for-Human-Action-Ji-Xu/3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f},
	abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
	urldate = {2018-09-06TZ},
	journal = {undefined},
	author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
	year = {2010}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2018-09-06TZ},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@inproceedings{karpathy_large-scale_2014,
	title = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	doi = {10.1109/CVPR.2014.223},
	abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Karpathy, A. and Toderici, G. and Shetty, S. and Leung, T. and Sukthankar, R. and Fei-Fei, L.},
	month = jun,
	year = {2014},
	keywords = {CNN, Computational modeling, Computer architecture, Feature extraction, Spatial resolution, Streaming media, Training, UCF-101 action recognition dataset, UCF-101 baseline model, YouTube videos, action, classification, convolutional, convolutional neural networks, dataset, feature-based baselines, image classification, image motion analysis, image recognition problems, large-scale, local spatiotemporal information, multimedia computing, network, neural, neural nets, recognition, social networking (online), spatiotemporal networks, spatiotemporal phenomena, sports, video, video classification, video signal processing},
	pages = {1725--1732}
}

@article{lan_beyond_2014,
	title = {Beyond {Gaussian} {Pyramid}: {Multi}-skip {Feature} {Stacking} for {Action} {Recognition}},
	shorttitle = {Beyond {Gaussian} {Pyramid}},
	url = {http://arxiv.org/abs/1411.6660},
	abstract = {Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost.},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1411.6660 [cs]},
	author = {Lan, Zhenzhong and Lin, Ming and Li, Xuanchong and Hauptmann, Alexander G. and Raj, Bhiksha},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.6660},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{peng_action_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Action {Recognition} with {Stacked} {Fisher} {Vectors}},
	isbn = {978-3-319-10602-1},
	abstract = {Representation of video is a vital problem in action recognition. This paper proposes Stacked Fisher Vectors (SFV), a new representation with multi-layer nested Fisher vector encoding, for action recognition. In the first layer, we densely sample large subvolumes from input videos, extract local features, and encode them using Fisher vectors (FVs). The second layer compresses the FVs of subvolumes obtained in previous layer, and then encodes them again with Fisher vectors. Compared with standard FV, SFV allows refining the representation and abstracting semantic information in a hierarchical way. Compared with recent mid-level based action representations, SFV need not to mine discriminative action parts but can preserve mid-level information through Fisher vector encoding in higher layer. We evaluate the proposed methods on three challenging datasets, namely Youtube, J-HMDB, and HMDB51. Experimental results demonstrate the effectiveness of SFV, and the combination of the traditional FV and SFV outperforms state-of-the-art methods on these datasets with a large margin.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Peng, Xiaojiang and Zou, Changqing and Qiao, Yu and Peng, Qiang},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Action recognition, Fisher vectors, max-margin dimensionality reduction, stacked Fisher vectors},
	pages = {581--595}
}

@article{peng_bag_2014,
	title = {Bag of {Visual} {Words} and {Fusion} {Methods} for {Action} {Recognition}: {Comprehensive} {Study} and {Good} {Practice}},
	shorttitle = {Bag of {Visual} {Words} and {Fusion} {Methods} for {Action} {Recognition}},
	url = {http://arxiv.org/abs/1405.4506},
	abstract = {Video based action recognition is one of the important and challenging problems in computer vision research. Bag of Visual Words model (BoVW) with local features has become the most popular method and obtained the state-of-the-art performance on several realistic datasets, such as the HMDB51, UCF50, and UCF101. BoVW is a general pipeline to construct a global representation from a set of local features, which is mainly composed of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. Many efforts have been made in each step independently in different scenarios and their effect on action recognition is still unknown. Meanwhile, video data exhibits different views of visual pattern, such as static appearance and motion dynamics. Multiple descriptors are usually extracted to represent these different views. Many feature fusion methods have been developed in other areas and their influence on action recognition has never been investigated before. This paper aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practice to produce a state-of-the-art action recognition system. Specifically, we explore two kinds of local features, ten kinds of encoding methods, eight kinds of pooling and normalization strategies, and three kinds of fusion methods. We conclude that every step is crucial for contributing to the final recognition rate. Furthermore, based on our comprehensive study, we propose a simple yet effective representation, called hybrid representation, by exploring the complementarity of different BoVW frameworks and local descriptors. Using this representation, we obtain the state-of-the-art on the three challenging datasets: HMDB51 (61.1\%), UCF50 (92.3\%), and UCF101 (87.9\%).},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1405.4506 [cs]},
	author = {Peng, Xiaojiang and Wang, Limin and Wang, Xingxing and Qiao, Yu},
	month = may,
	year = {2014},
	note = {arXiv: 1405.4506},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	number = {Nov},
	urldate = {2018-09-06TZ},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605}
}

@misc{noauthor_first_nodate,
	title = {The {First} {International} {Workshop} on {Action} {Recognition} with a {Large} {Number} of {Classes}, {Sydney}, {Australia}},
	url = {http://crcv.ucf.edu/ICCV13-Action-Workshop/},
	urldate = {2018-09-06TZ}
}

@inproceedings{farneback_two-frame_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Two-{Frame} {Motion} {Estimation} {Based} on {Polynomial} {Expansion}},
	isbn = {978-3-540-45103-7},
	abstract = {This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.},
	language = {en},
	booktitle = {Image {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Farnebäck, Gunnar},
	editor = {Bigun, Josef and Gustavsson, Tomas},
	year = {2003},
	keywords = {Computer Vision, Motion Model, Orientation Tensor, Polynomial Expansion, Quadratic Polynomial},
	pages = {363--370}
}

@inproceedings{wang_action_2013,
	title = {Action {Recognition} with {Improved} {Trajectories}},
	doi = {10.1109/ICCV.2013.441},
	abstract = {Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Wang, H. and Schmid, C.},
	month = dec,
	year = {2013},
	keywords = {Adaptive optics, Cameras, Detectors, Feature extraction, HOF, MBH, Optical imaging, RANSAC, SURF descriptors, Trajectory, Vectors, action recognition, camera motion estimation, cameras, dense optical flow, dense trajectory, feature point matching, human detector, human motion, image matching, image representation, image sequences, motion estimation, motion-based descriptors, video representation, video signal processing},
	pages = {3551--3558}
}

@inproceedings{wang_action_2011,
	title = {Action recognition by dense trajectories},
	doi = {10.1109/CVPR.2011.5995407},
	abstract = {Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports.},
	booktitle = {{CVPR} 2011},
	author = {Wang, H. and Kläser, A. and Schmid, C. and Liu, C.},
	month = jun,
	year = {2011},
	keywords = {Cameras, Feature extraction, KLT tracker, Optical imaging, SIFT descriptor matching, Tracking, Training, Trajectory, Videos, action recognition, bag-of-features approach, camera motion, cameras, dense trajectories, feature extraction, feature trajectories, image classification, image motion analysis, image sampling, image sequences, motion boundary histograms, optical flow field, uncontrolled realistic videos},
	pages = {3169--3176}
}

@article{wang_action_2015,
	title = {Action {Recognition} with {Trajectory}-{Pooled} {Deep}-{Convolutional} {Descriptors}},
	url = {http://arxiv.org/abs/1505.04868},
	doi = {10.1109/CVPR.2015.7299059},
	abstract = {Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets (HMDB51 65.9\%, UCF101 91.5\%).},
	urldate = {2018-09-06TZ},
	journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Wang, Limin and Qiao, Yu and Tang, Xiaoou},
	month = jun,
	year = {2015},
	note = {arXiv: 1505.04868},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {4305--4314}
}

@article{tran_learning_2014-1,
	title = {Learning {Spatiotemporal} {Features} with 3D {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1412.0767},
	abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1412.0767 [cs]},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.0767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-09-06TZ},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	urldate = {2018-09-05TZ},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05027},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@article{xie_aggregated_2016,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	urldate = {2018-09-05TZ},
	journal = {arXiv:1611.05431 [cs]},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{noauthor_cs231n_nodate,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/},
	urldate = {2018-09-05TZ}
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {University} {CS}231n: {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.stanford.edu/},
	urldate = {2018-09-05TZ}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {2D shape variability, Character recognition, Feature extraction, GTN, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, performance measure minimization, segmentation recognition},
	pages = {2278--2324}
}

@article{fukushima_neocognitron_2007,
	title = {Neocognitron},
	volume = {2},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Neocognitron},
	doi = {10.4249/scholarpedia.1717},
	language = {en},
	number = {1},
	urldate = {2018-09-04TZ},
	journal = {Scholarpedia},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {2007},
	pages = {1717}
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {0022-3751},
	abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
	language = {eng},
	number = {1},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = mar,
	year = {1968},
	pmid = {4966457},
	pmcid = {PMC1557912},
	keywords = {Animals, Color Perception, Evoked Potentials, Haplorhini, Light, Motion Perception, Occipital Lobe, Retina, Vision, Ocular, Visual Fields},
	pages = {215--243}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-03561-3},
	abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@article{dardas_real-time_2011,
	title = {Real-{Time} {Hand} {Gesture} {Detection} and {Recognition} {Using} {Bag}-of-{Features} and {Support} {Vector} {Machine} {Techniques}},
	volume = {60},
	issn = {0018-9456},
	doi = {10.1109/TIM.2011.2161140},
	abstract = {This paper presents a novel and real-time system for interaction with an application or video game via hand gestures. Our system includes detecting and tracking bare hand in cluttered background using skin detection and hand posture contour comparison algorithm after face subtraction, recognizing hand gestures via bag-of-features and multiclass support vector machine (SVM) and building a grammar that generates gesture commands to control an application. In the training stage, after extracting the keypoints for every training image using the scale invariance feature transform (SIFT), a vector quantization technique will map keypoints from every training image into a unified dimensional histogram vector (bag-of-words) after K-means clustering. This histogram is treated as an input vector for a multiclass SVM to build the training classifier. In the testing stage, for every frame captured from a webcam, the hand is detected using our algorithm, then, the keypoints are extracted for every small image that contains the detected hand gesture only and fed into the cluster model to map them into a bag-of-words vector, which is finally fed into the multiclass SVM training classifier to recognize the hand gesture.},
	number = {11},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Dardas, N. H. and Georganas, N. D.},
	month = nov,
	year = {2011},
	keywords = {Bag-of-features, Feature extraction, Gesture recognition, Grammar, Human computer interaction, K-means, Object detection, Object recognition, Real time systems, Support vector machines, bag-of-features, computer games, face subtraction, gesture recognition, grammar, hand gesture, hand gesture detection, hand gesture recognition, hand posture, hand posture contour comparison algorithm, human computer interaction, image classification, k-means clustering, keypoint extraction, multiclass SVM training classifier, object detection, object recognition, pattern clustering, real-time system, scale invariance feature transform, scale invariant feature transform (SIFT), skin, skin detection, support vector machine (SVM), support vector machine techniques, support vector machines, unified dimensional histogram vector, vector quantisation, vector quantization technique, video game},
	pages = {3592--3607}
}

@inproceedings{pan_real-time_2010,
	title = {A real-time multi-cue hand tracking algorithm based on computer vision},
	doi = {10.1109/VR.2010.5444787},
	abstract = {Although hand tracking algorithm has been widely used in virtual reality and HCI system, it is still a challenging problem in vision-based research area. Due to the robustness and real-time requirements in VR applications, most hand tracking algorithms require special device to achieve satisfactory results. In this paper, we propose an easy-to-use and inexpensive approach to track the hands accurately with a single normal webcam. Outstretched hand is detected by contour \& curvature based detection techniques to initialize the tracking region. Robust multi-cue hand tracking is then achieved by velocity-weighted features and color cue. Experiments show that the proposed multi-cue hand tracking approach achieves continuous real-time results even for the situation of cluttered background. The approach fulfills the speed and accuracy requirements of frontal-view vision-based human computer interactions.},
	booktitle = {2010 {IEEE} {Virtual} {Reality} {Conference} ({VR})},
	author = {Pan, Z. and Li, Y. and Zhang, M. and Sun, C. and Guo, K. and Tang, X. and Zhou, S. Z.},
	month = mar,
	year = {2010},
	keywords = {3D Interaction, Application software, Cameras, Chaos, Computer vision, Gesture detection, HCI system, Hand tracking, Human computer interaction, Object detection, Real time systems, Robustness, Skin, Virtual reality, color cue, computer vision, contour based detection, curvature based detection, frontal view vision based human computer interactions, human computer interaction, real-time systems, realtime multicue hand tracking algorithm, robust multicue hand tracking, tracking, virtual reality},
	pages = {219--222}
}

@inproceedings{bergh_haarlet-based_2009,
	title = {Haarlet-based hand gesture recognition for 3D interaction},
	doi = {10.1109/WACV.2009.5403103},
	abstract = {This paper presents a system to manipulate 3D objects or navigate through 3D models by detecting the gestures and the movements of the hands of a user in front of a camera mounted on top of a screen. This paper more particularly introduces an improved skin color segmentation algorithm which combines an online and an offline model; and a Haarlet-based hand gesture recognition system, where the Haarlets are trained based on Average Neighborhood Margin Maximization (ANMM). The result is a real-time marker-less interaction system which is applied to two applications, one for manipulating 3D objects, and the other for navigating through a 3D model.},
	booktitle = {2009 {Workshop} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Bergh, M. Van den and Koller-Meier, E. and Bosché, F. and Gool, L. Van},
	month = dec,
	year = {2009},
	keywords = {3D interaction, 3D models, 3D objects, Cameras, Haarlet-based hand gesture recognition, Navigation, Object detection, Real time systems, Skin, average neighborhood margin maximization, gesture recognition, image colour analysis, image segmentation, improved skin color segmentation algorithm, offline model, online model, optimisation, real-time marker-less interaction system, real-time systems, solid modelling},
	pages = {1--8}
}

@article{pavlovic_visual_1997,
	title = {Visual interpretation of hand gestures for human-computer interaction: a review},
	volume = {19},
	issn = {0162-8828},
	shorttitle = {Visual interpretation of hand gestures for human-computer interaction},
	doi = {10.1109/34.598226},
	abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient "purposive" approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of human-computer interaction.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Pavlovic, V. I. and Sharma, R. and Huang, T. S.},
	month = jul,
	year = {1997},
	keywords = {Communications technology, Computational modeling, Computer displays, Computer vision, HCI, Human computer interaction, Keyboards, Motion analysis, Potential well, Tracking, Virtual reality, computationally efficient purposive approaches, hand gestures, human-computer interaction, image recognition, motion estimation, real-time requirements, reviews, user interfaces, vision-based gesture recognition, visual interpretation},
	pages = {677--695}
}

@inproceedings{kolsch_robust_2004,
	title = {Robust hand detection},
	doi = {10.1109/AFGR.2004.1301601},
	abstract = {Vision-based hand gesture interfaces require fast and extremely robust hand detection. Here, we study view-specific hand posture detection with an object recognition method proposed by Viola and Jones. Training with this method is computationally very expensive, prohibiting the evaluation of many hand appearances for their suitability to detection. In this paper, we present a frequency analysis-based method for instantaneous estimation of class separability, without the need for any training. We built detectors for the most promising candidates, their receiver operating characteristics confirming the estimates. Next, we found that classification accuracy increases with a more expressive feature type. Lastly, we show that further optimization of training parameters yields additional detection rate improvements. In summary, we present a systematic approach to building an extremely robust hand appearance detector, providing an important step towards easily deployable and reliable vision-based hand gesture interfaces.},
	booktitle = {Sixth {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition}, 2004. {Proceedings}.},
	author = {Kolsch, M. and Turk, M.},
	month = may,
	year = {2004},
	keywords = {Face detection, Face recognition, Robustness, computer vision, frequency analysis-based method, gesture recognition, hand posture detection, object recognition, object recognition method, optimisation, robust hand detection, vision-based hand gesture interfaces},
	pages = {614--619}
}

@misc{noauthor_real-time_nodate,
	title = {Real-{Time} {Hand} {Gesture} {Detection} and {Recognition} {Using} {Bag}-of-{Features} and {Support} {Vector} {Machine} {Techniques} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/document/5983442/},
	urldate = {2018-08-29TZ}
}

@misc{noauthor_real-time_nodate-1,
	title = {A real-time multi-cue hand tracking algorithm based on computer vision - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/document/5444787/},
	urldate = {2018-08-29TZ}
}

@misc{noauthor_haarlet-based_nodate,
	title = {Haarlet-based hand gesture recognition for 3D interaction - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/document/5403103/},
	urldate = {2018-08-29TZ}
}

@misc{noauthor_visual_nodate,
	title = {Visual interpretation of hand gestures for human-computer interaction: a review - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/document/598226/},
	urldate = {2018-08-29TZ}
}

@inproceedings{jiang_dynamic_2013,
	title = {A dynamic gesture recognition method based on computer vision},
	volume = {2},
	doi = {10.1109/CISP.2013.6745246},
	abstract = {In recent years, the development of human-computer interaction (HCI) techniques is very fast and the typical application is the gesture interaction technology. This paper proposes a useful and robust dynamic gesture recognition method. The first step is to detect hand in every image frame obtained from a USB camera, through skin segmentation and hand feature extraction. Hand region is segmented based on YCrCb color space and detected by detecting the number of fingers. Then, the algorithm based on ellipse fitting and motion feature is used to track hand. We recognize the trajectory of hand obtained after tracking based on a simple method. The experimental results show that the proposed method is reliable and robust.},
	booktitle = {2013 6th {International} {Congress} on {Image} and {Signal} {Processing} ({CISP})},
	author = {Jiang, X. and Lu, X. and Chen, L. and Zhou, L. and Shen, S.},
	month = dec,
	year = {2013},
	keywords = {Dynamic Gesture Recognition, Ellipse Fitting, Fitting, Gesture recognition, HCI techniques, Hand Feature Extraction, Hand Tracking, Image color analysis, Skin, Skin Segmentation, Target tracking, Trajectory, USB camera, YCrCb color space, computer vision, dynamic gesture recognition method, ellipse fitting, feature extraction, gesture interaction technology, gesture recognition, hand feature extraction, hand region, human computer interaction, human-computer interaction, image colour analysis, image frame, image sensors, skin segmentation},
	pages = {646--650}
}
@misc{noauthor_rgb_nodate,
	title = {{RGB} {Representation} in {Computers}},
	url = {https://cdn-images-1.medium.com/max/1600/0*I6XPXr9fzJ43L0ii.gif},
	urldate = {2018-08-28TZ}
}

@article{papert_summer_1966,
	title = {The {Summer} {Vision} {Project}},
	url = {http://dspace.mit.edu/handle/1721.1/6125},
	abstract = {The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which allow individuals to work independently and yet participate in the construction of a system complex enough to be real landmark in the development of "pattern recognition". The basic structure is fixed for the first phase of work extending to some point in July. Everyone is invited to contribute to the discussion of the second phase. Sussman is coordinator of "Vision Project" meetings and should be consulted by anyone who wishes to participate. The primary goal of the project is to construct a system of programs which will divide a vidisector picture into regions such as likely objects, likely background areas and chaos. We shall call this part of its operation FIGURE-GROUND analysis. It will be impossible to do this without considerable analysis of shape and surface properties, so FIGURE-GROUND analysis is really inseparable in practice from the second goal which is REGION DESCRIPTION. The final goal is OBJECT IDENTIFICATION which will actually name objects by matching them with a vocabulary of known objects.},
	language = {en\_US},
	urldate = {2018-08-28TZ},
	author = {Papert, Seymour A.},
	month = jul,
	year = {1966}
}

@inproceedings{wen_intraoperative_2010,
	title = {Intraoperative {Visual} {Guidance} and {Control} {Interface} for {Augmented} {Reality} {Robotic} {Surgery}},
	doi = {10.1109/ICCA.2010.5524421},
	abstract = {We proposed an augmented reality (AR) robotic system equipped with intraoperative visual guidance and gesture based control interface. The proposed feature is an enhancement of our AR robotic system. AR robotic system was introduced to the field of interventional medicine to assist surgeons in implementing medical operations under the augmented reality environment. The technique combining dextrous robot and AR guidance provides a new operational mode for surgeons. The introduction of robotic modules is to compliment surgeon's dexterity and to perform specific tasks defined during the surgery. Augmented reality provides additional visualization and interaction absence in a typical clinical environment. The interfacing system includes intraoperative tracking of surgical tools and virtual reconstruction of the visually occluded tool segment, and an intuitive gesture-based human-computer interaction centered on the projected organ. Preliminary experiments show that this novel human-machine interface is effective for surgical intervention.},
	booktitle = {2010 8th {IEEE} {International} {Conference} on {Control} and {Automation}, {ICCA} 2010},
	author = {Wen, Rong and Yang, Liangjing and Chui, Chee-Kong and Lim, Kah-Bin and Chang, Sha},
	month = jul,
	year = {2010},
	pages = {947--952}
}

@inproceedings{abhishek_glove-based_2016,
	title = {Glove-based hand gesture recognition sign language translator using capacitive touch sensor},
	doi = {10.1109/EDSSC.2016.7785276},
	abstract = {The sign language translator is a bridge between those who comprehend sign languages and those who do not which is the majority of humanity. However, conventional signa language translators are bulky and expensive, limiting their wide adoption. In this paper, we present a gesture recognition glove based on charge-transfer touch sensors for the translation of the American Sign Language. The device is portable and can be implemented with low-cost hardware. The prototype recognize gestures for the numbers 0 to 9 and the 26 English alphabets, A to Z. The glove experimentally achieved, based on 1080 trials, an overall detection accuracies of over 92 \%, which is comparable with current high-end counterparts. The proposed device i expected to bridge the communication gap between the hearing and speech impaired and members of the general public.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Electron} {Devices} and {Solid}-{State} {Circuits} ({EDSSC})},
	author = {Abhishek, K. S. and Qubeley, L. C. F. and Ho, D.},
	month = aug,
	year = {2016},
	keywords = {American sign language, Conferences, Electron devices, Glove, Hafnium, Sensors, Solid state circuits, capacitive sensor, capacitive sensors, capacitive touch sensor, charge-transfer touch sensors, gesture recognition, glove-based hand gesture recognition, language translation, low-cost hardware, palmprint recognition, portable, sign language translator, tactile sensors},
	pages = {334--337}
}

@inproceedings{card_information_1991,
	address = {New York, NY, USA},
	series = {{CHI} '91},
	title = {The {Information} {Visualizer}, an {Information} {Workspace}},
	isbn = {978-0-89791-383-6},
	url = {http://doi.acm.org/10.1145/108844.108874},
	doi = {10.1145/108844.108874},
	urldate = {2018-08-27TZ},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Card, Stuart K. and Robertson, George G. and Mackinlay, Jock D.},
	year = {1991},
	pages = {181--186}
}

@inproceedings{miller_response_1968,
	address = {New York, NY, USA},
	series = {{AFIPS} '68 ({Fall}, part {I})},
	title = {Response {Time} in {Man}-computer {Conversational} {Transactions}},
	url = {http://doi.acm.org/10.1145/1476589.1476628},
	doi = {10.1145/1476589.1476628},
	abstract = {The literature concerning man-computer transactions abounds in controversy about the limits of "system response time" to a user's command or inquiry at a terminal. Two major semantic issues prohibit resolving this controversy. One issue centers around the question of "Response time to what?" The implication is that different human purposes and actions will have different acceptable or useful response times.},
	urldate = {2018-08-27TZ},
	booktitle = {Proceedings of the {December} 9-11, 1968, {Fall} {Joint} {Computer} {Conference}, {Part} {I}},
	publisher = {ACM},
	author = {Miller, Robert B.},
	year = {1968},
	pages = {267--277}
}

@misc{noauthor_gesture_nodate,
	title = {gesture recognition {Meaning} in the {Cambridge} {English} {Dictionary}},
	url = {https://dictionary.cambridge.org/dictionary/english/gesture-recognition},
	abstract = {gesture recognition definition: the ability of a computer to react to movements made by a human being and to do tasks according to what the movements mean: . Learn more.},
	language = {en},
	urldate = {2018-08-27TZ}
}

@article{tran_learning_2014,
	title = {Learning {Spatiotemporal} {Features} with 3D {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1412.0767},
	abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
	urldate = {2018-08-27TZ},
	journal = {arXiv:1412.0767 [cs]},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.0767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-08-27TZ},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wei_role_2013,
	title = {The {Role} of {Balanced} {Training} and {Testing} {Data} {Sets} for {Binary} {Classifiers} in {Bioinformatics}},
	volume = {8},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706434/},
	doi = {10.1371/journal.pone.0067863},
	abstract = {Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50\% neutral and 50\% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand.},
	number = {7},
	urldate = {2018-08-27TZ},
	journal = {PLoS ONE},
	author = {Wei, Qiong and Dunbrack, Roland L.},
	month = jul,
	year = {2013},
	pmid = {23874456},
	pmcid = {PMC3706434}
}

@article{tran_closer_2017,
	title = {A {Closer} {Look} at {Spatiotemporal} {Convolutions} for {Action} {Recognition}},
	url = {http://arxiv.org/abs/1711.11248},
	abstract = {In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional ﬁlters into separate spatial and temporal components yields signiﬁcantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block “R(2+1)D” which produces CNNs that achieve results comparable or superior to the state-of-theart on Sports-1M, Kinetics, UCF101, and HMDB51.},
	language = {en},
	urldate = {2018-08-27TZ},
	journal = {arXiv:1711.11248 [cs]},
	author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11248},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{molchanov_online_2016,
	address = {Las Vegas, NV, USA},
	title = {Online {Detection} and {Classification} of {Dynamic} {Hand} {Gestures} with {Recurrent} 3D {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780825/},
	doi = {10.1109/CVPR.2016.456},
	abstract = {Automatic detection and classiﬁcation of dynamic hand gestures in real-world systems intended for human computer interaction is challenging as: 1) there is a large diversity in how people perform gestures, making detection and classiﬁcation difﬁcult; 2) the system must work online in order to avoid noticeable lag between performing a gesture and its classiﬁcation; in fact, a negative lag (classiﬁcation before the gesture is ﬁnished) is desirable, as feedback to the user can then be truly instantaneous. In this paper, we address these challenges with a recurrent three-dimensional convolutional neural network that performs simultaneous detection and classiﬁcation of dynamic hand gestures from multi-modal data. We employ connectionist temporal classiﬁcation to train the network to predict class labels from inprogress gestures in unsegmented input streams. In order to validate our method, we introduce a new challenging multimodal dynamic hand gesture dataset captured with depth, color and stereo-IR sensors. On this challenging dataset, our gesture recognition system achieves an accuracy of 83.8\%, outperforms competing state-of-the-art algorithms, and approaches human accuracy of 88.4\%. Moreover, our method achieves state-of-the-art performance on SKIG and ChaLearn2014 benchmarks.},
	language = {en},
	urldate = {2018-08-25TZ},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
	month = jun,
	year = {2016},
	pages = {4207--4215}
}

@misc{twentybn_20bn-jester_nodate,
	title = {The 20BN-{JESTER} {Dataset} {\textbar} {TwentyBN}},
	url = {https://20bn.com/datasets/jester},
	urldate = {2018-08-25TZ},
	author = {TwentyBN}
}

@article{soomro_ucf101:_2012,
	title = {{UCF}101: {A} {Dataset} of 101 {Human} {Actions} {Classes} {From} {Videos} in {The} {Wild}},
	shorttitle = {{UCF}101},
	url = {https://arxiv.org/abs/1212.0402},
	language = {en},
	urldate = {2018-08-25TZ},
	author = {Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
	month = dec,
	year = {2012}
}

@inproceedings{kuehne_hmdb51:_2011,
	title = {{HMDB}51: {A} {Large} {Video} {Database} for {Human} {Motion} {Recognition}},
	shorttitle = {{HMDB}51},
	doi = {10.1109/ICCV.2011.6126543},
	abstract = {With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Kuehne, Hilde and Jhuang, Hueihan and Garrote, Estibaliz and Poggio, Tomaso and Serre, Thomas},
	month = nov,
	year = {2011},
	pages = {2556--2563}
}

@article{kay_kinetics_2017,
	title = {The {Kinetics} {Human} {Action} {Video} {Dataset}},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	urldate = {2018-08-25TZ},
	journal = {arXiv:1705.06950 [cs]},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{goyal_something_2017,
	title = {The "something something" video database for learning and evaluating visual common sense},
	url = {http://arxiv.org/abs/1706.04261},
	abstract = {Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the "something-something" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.},
	urldate = {2018-08-25TZ},
	journal = {arXiv:1706.04261 [cs]},
	author = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzyńska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and Hoppe, Florian and Thurau, Christian and Bax, Ingo and Memisevic, Roland},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{heilbron_activitynet:_2015,
	address = {Boston, MA, USA},
	title = {{ActivityNet}: {A} large-scale video benchmark for human activity understanding},
	isbn = {978-1-4673-6964-0},
	shorttitle = {{ActivityNet}},
	url = {http://ieeexplore.ieee.org/document/7298698/},
	doi = {10.1109/CVPR.2015.7298698},
	abstract = {In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper we introduce ActivityNet, a new largescale video benchmark for human activity understanding. Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: untrimmed video classiﬁcation, trimmed activity classiﬁcation and activity detection.},
	language = {en},
	urldate = {2018-08-25TZ},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
	month = jun,
	year = {2015},
	pages = {961--970}
}

@misc{noauthor_participant_nodate,
	title = {Participant {Agreement}},
	url = {https://docs.google.com/forms/d/e/1FAIpQLSc7ZcohjasKVwKszhISAH7DHWi8ElounQd1oZwORkSFzrdKbg/viewform?usp=embed_facebook},
	abstract = {NVIDIA Dynamic Hand Gesture Dataset

Purpose
The NVIDIA Dynamic Hand Gesture Dataset is a resource for researchers in the field of dynamic hand gesture recognition. The database was acquired at NVIDIA Corporation, 2701 San Tomas Expressway, Santa Clara, CA 95050, USA (hereafter NVIDIA). NVIDIA retains complete copyright ownership of the hand gesture (depth, color, and infra-red stereo pair videos and ground truth labels) data. NVIDIA makes the database publicly available solely for the purposes of research subject to the following restrictions.

Restrictions
User agrees to the following restrictions on use of videos and data from the NVIDIA Dynamic Hand Gesture Dataset:
1. Usage: The NVIDIA Dynamic Hand Gesture Dataset is intended for research purposes only and may not be used for any commercial purposes. The NVIDIA Dynamic Hand Gesture Dataset is to be used only for research (development and testing) on algorithms for automated dynamic hand gesture recognition. The videos in the database must not be used in any way that could compromise the dignity of the human subject depicted in the image.

2. Commercialization: The User will not promote or sell any part of the contents of the NVIDIA Dynamic Hand Gesture Dataset, or associate any phrase or logo with the contents of the NVIDIA Dynamic Hand Gesture Dataset, either directly or indirectly. The NVIDIA Dynamic Hand Gesture Dataset is distributed on a not-for-profit basis and the User agrees to make no for-profit use or commercialization of the database without the written permission of legal counsel from NVIDIA.

3. Duplication: The User is granted access to the database only for the purpose of their own internal research work. The User will not duplicate, share, publish, modify or copy the NVIDIA Dynamic Hand Gesture Dataset in any way, even with other research or development groups within User’s organization or company. The User will not publish digital copies of any part of the NVIDIA Dynamic Hand Gesture Dataset on any digital recording medium, on any Internet web site or ftp site, or by email, without consent from NVIDIA.

4. Citations: All manuscripts that are submitted for publication to any publisher or journal or conference or other venue, or that are posted on any Internet website or ftp site or sent by email, and that use images or contain images from the NVIDIA Dynamic Hand Gesture Dataset, or that report on research results that made use of the videos or data within the NVIDIA Dynamic Hand Gesture Dataset, must make the following appropriate citation to the database.

P Molchanov, X Yang, S Gupta, K Kim, S Tyree and J Kautz, “Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks”, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016, June 2016.

Any such publication may not contain more than 5 of the images in the NVIDIA Dynamic Hand Gesture Dataset. Further, all such publications must be sent in electronic format to NVIDIA at: r3dcnn@nvidia.com upon public release or publication.

5. Indemnification: NVIDIA gives no warranty as to the accuracy, identity, or verifiability of any of the videos or data within the NVIDIA Dynamic Hand Gesture Dataset. User agrees to indemnify, defend, and hold harmless NVIDIA and its Board of Directors, officers, employees and agents, individually and collectively, from any and all losses, expenses, damages, demands and/or claims based upon any such injury or damage (real or alleged) and shall pay all damages, claims, judgments or expenses resulting from User’s use of the NVIDIA Dynamic Hand Gesture Dataset.

The User must abide by the provisions of this Participant Agreement or permission to use the images and data will be revoked. The NVIDIA Dynamic Hand Gesture Dataset will be made available to User via an Internet site revealed upon acceptance of this Participant agreement by the User. There will be no charge for data made available and downloaded via the Internet.},
	urldate = {2018-08-25TZ},
	journal = {Google Docs}
}

@misc{nvidia_online_nodate,
	title = {Online {Detection} and {Classification} of {Dynamic} {Hand} {Gestures} with {Recurrent} 3D {Convolutional} {Neural} {Networks} {\textbar} {Research}},
	url = {https://research.nvidia.com/publication/online-detection-and-classification-dynamic-hand-gestures-recurrent-3d-convolutional},
	urldate = {2018-08-25TZ},
	author = {NVIDIA}
}

@misc{noauthor_egogesture_nodate,
	title = {{EgoGesture} {Dataset}},
	url = {http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html},
	urldate = {2018-08-25TZ}
}

@article{zhang_egogesture:_2018,
	title = {{EgoGesture}: {A} {New} {Dataset} and {Benchmark} for {Egocentric} {Hand} {Gesture} {Recognition}},
	volume = {20},
	issn = {1520-9210},
	shorttitle = {{EgoGesture}},
	doi = {10.1109/TMM.2018.2808769},
	abstract = {Gesture is a natural interface in human-computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24 000 gesture samples and 3 000 000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
	number = {5},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Y. and Cao, C. and Cheng, J. and Lu, H.},
	month = may,
	year = {2018},
	keywords = {Benchmark, Benchmark testing, Cameras, EgoGesture, Gesture recognition, Neural networks, Performance evaluation, Task analysis, Three-dimensional displays, benchmark dataset, dataset, deep learning era, deep neural networks, diverse indoor scenes, dynamic gestures, egocentric hand gesture recognition, egocentric vision, first-person view, gesture classification, gesture recognition, gesture recognition community, gesture samples, gesture spotting, human-computer interaction, learning (artificial intelligence), natural interface, neural nets, outdoor scenes, wearable devices},
	pages = {1038--1050}
}

@article{aggarwal_human_2014,
	series = {Celebrating the life and work of {Maria} {Petrou}},
	title = {Human activity recognition from 3D data: {A} review},
	volume = {48},
	issn = {0167-8655},
	shorttitle = {Human activity recognition from 3D data},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865514001299},
	doi = {10.1016/j.patrec.2014.04.011},
	abstract = {Human activity recognition has been an important area of computer vision research since the 1980s. Various approaches have been proposed with a great portion of them addressing this issue via conventional cameras. The past decade has witnessed a rapid development of 3D data acquisition techniques. This paper summarizes the major techniques in human activity recognition from 3D data with a focus on techniques that use depth data. Broad categories of algorithms are identified based upon the use of different features. The pros and cons of the algorithms in each category are analyzed and the possible direction of future research is indicated.},
	urldate = {2018-08-24TZ},
	journal = {Pattern Recognition Letters},
	author = {Aggarwal, J. K. and Xia, Lu},
	month = oct,
	year = {2014},
	keywords = {3D data, Computer vision, Depth image, Human activity recognition},
	pages = {70--80}
}

@inproceedings{li_action_2010,
	title = {Action recognition based on a bag of 3D points},
	doi = {10.1109/CVPRW.2010.5543273},
	abstract = {This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90\% recognition accuracy were achieved by sampling only about 1\% 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Workshops}},
	author = {Li, W. and Zhang, Z. and Liu, Z.},
	month = jun,
	year = {2010},
	keywords = {3D points, Australia, Cameras, Computational modeling, Computer vision, Humans, Joints, Pattern recognition, Sampling methods, Shape, Video sequences, action graph, gesture recognition, graph theory, hidden feature removal, human action recognition, human motion, motion estimation, occlusions, projection based sampling scheme},
	pages = {9--14}
}

@inproceedings{ruffieux_survey_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Survey} of {Datasets} for {Human} {Gesture} {Recognition}},
	isbn = {978-3-319-07230-2},
	abstract = {This paper presents a survey on datasets created for the field of gesture recognition. The main characteristics of the datasets are presented on two tables to provide researchers a clear and rapid access to the information. This paper also provides a comprehensive description of the datasets and discusses their general strengths and limitations. Guidelines for creation and selection of datasets for gesture recognition are proposed. This survey should be a key-access point for researchers looking to create or use datasets in the field of human gesture recognition.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Advanced} {Interaction} {Modalities} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Ruffieux, Simon and Lalanne, Denis and Mugellini, Elena and Abou Khaled, Omar},
	editor = {Kurosu, Masaaki},
	year = {2014},
	keywords = {datasets, gesture recognition, human-computer interaction, survey},
	pages = {337--348}
}

@inproceedings{ruffieux_survey_2014-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Survey} of {Datasets} for {Human} {Gesture} {Recognition}},
	isbn = {978-3-319-07230-2},
	abstract = {This paper presents a survey on datasets created for the field of gesture recognition. The main characteristics of the datasets are presented on two tables to provide researchers a clear and rapid access to the information. This paper also provides a comprehensive description of the datasets and discusses their general strengths and limitations. Guidelines for creation and selection of datasets for gesture recognition are proposed. This survey should be a key-access point for researchers looking to create or use datasets in the field of human gesture recognition.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Advanced} {Interaction} {Modalities} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Ruffieux, Simon and Lalanne, Denis and Mugellini, Elena and Abou Khaled, Omar},
	editor = {Kurosu, Masaaki},
	year = {2014},
	keywords = {datasets, gesture recognition, human-computer interaction, survey},
	pages = {337--348}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {A} {Survey} of {Datasets} for {Human} {Gesture} {Recognition}},
	url = {https://www.researchgate.net/publication/276242548_A_Survey_of_Datasets_for_Human_Gesture_Recognition},
	urldate = {2018-08-24TZ}
}

@article{chaquet_survey_2013,
	title = {A survey of video datasets for human action and activity recognition},
	volume = {117},
	issn = {1077-3142},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314213000295},
	doi = {10.1016/j.cviu.2013.01.013},
	abstract = {Vision-based human action and activity recognition has an increasing importance among the computer vision community with applications to visual surveillance, video retrieval and human–computer interaction. In recent years, more and more datasets dedicated to human action and activity recognition have been created. The use of these datasets allows us to compare different recognition systems with the same input data. The survey introduced in this paper tries to cover the lack of a complete description of the most important public datasets for video-based human activity and action recognition and to guide researchers in the election of the most suitable dataset for benchmarking their algorithms.},
	number = {6},
	urldate = {2018-08-24TZ},
	journal = {Computer Vision and Image Understanding},
	author = {Chaquet, Jose M. and Carmona, Enrique J. and Fernández-Caballero, Antonio},
	month = jun,
	year = {2013},
	keywords = {Database, Dataset, Human action recognition, Human activity recognition, Review, Survey},
	pages = {633--659}
}

@inproceedings{hassner_critical_2013,
	title = {A {Critical} {Review} of {Action} {Recognition} {Benchmarks}},
	doi = {10.1109/CVPRW.2013.43},
	abstract = {Understanding human actions in videos has been a central research theme in Computer Vision for decades, and much progress has been achieved over the years. Much of this progress was demonstrated on standard benchmarks used to evaluate novel techniques. These benchmarks and their evolution, provide a unique perspective on the growing capabilities of computerized action recognition systems. They demonstrate just how far machine vision systems have come while also underscore the gap that still remains between existing state-of-the-art performance and the needs of real-world applications. In this paper we provide a comprehensive survey of these benchmarks: from early examples, such as the Weizmann set, to recently presented, contemporary benchmarks. This paper further provides a summary of the results obtained in the last couple of years on the recent ASLAN benchmark, which was designed to reflect the many challenges modern Action Recognition systems are expected to overcome.},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Hassner, T.},
	month = jun,
	year = {2013},
	keywords = {ASLAN benchmark, Benchmark testing, Cameras, Computer vision, Motion pictures, TV, Videos, YouTube, action similarity labeling benchmark, computer vision, computerized human action recognition systems, digital videos, image recognition, machine vision systems, video signal processing},
	pages = {245--250}
}

@article{guo_survey_2014,
	title = {A survey on still image based human action recognition},
	volume = {47},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320314001642},
	doi = {10.1016/j.patcog.2014.04.018},
	abstract = {Recently still image-based human action recognition has become an active research topic in computer vision and pattern recognition. It focuses on identifying a person׳s action or behavior from a single image. Unlike the traditional action recognition approaches where videos or image sequences are used, a still image contains no temporal information for action characterization. Thus the prevailing spatiotemporal features for video-based action analysis are not appropriate for still image-based action recognition. It is more challenging to perform still image-based action recognition than the video-based one, given the limited source of information as well as the cluttered background for images collected from the Internet. On the other hand, a large number of still images exist over the Internet. Therefore it is demanding to develop robust and efficient methods for still image-based action recognition to understand the web images better for image retrieval or search. Based on the emerging research in recent years, it is time to review the existing approaches to still image-based action recognition and inspire more efforts to advance the field of research. We present a detailed overview of the state-of-the-art methods for still image-based action recognition, and categorize and describe various high-level cues and low-level features for action analysis in still images. All related databases are introduced with details. Finally, we give our views and thoughts for future research.},
	number = {10},
	urldate = {2018-08-24TZ},
	journal = {Pattern Recognition},
	author = {Guo, Guodong and Lai, Alice},
	month = oct,
	year = {2014},
	keywords = {Action recognition, Databases, Evaluation, Still image based, Survey, Various cues},
	pages = {3343--3361}
}

@article{wang_recent_2003,
	title = {Recent developments in human motion analysis},
	volume = {36},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320302001000},
	doi = {10.1016/S0031-3203(02)00100-0},
	abstract = {Visual analysis of human motion is currently one of the most active research topics in computer vision. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Human motion analysis concerns the detection, tracking and recognition of people, and more generally, the understanding of human behaviors, from image sequences involving humans. This paper provides a comprehensive survey of research on computer-vision-based human motion analysis. The emphasis is on three major issues involved in a general human motion analysis system, namely human detection, tracking and activity understanding. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are discussed.},
	number = {3},
	urldate = {2018-08-24TZ},
	journal = {Pattern Recognition},
	author = {Wang, Liang and Hu, Weiming and Tan, Tieniu},
	month = mar,
	year = {2003},
	keywords = {Behavior understanding, Detection, Human motion analysis, Semantic description, Tracking},
	pages = {585--601}
}

@article{lim_fuzzy_2015,
	title = {Fuzzy human motion analysis: {A} review},
	volume = {48},
	issn = {00313203},
	shorttitle = {Fuzzy human motion analysis},
	url = {http://arxiv.org/abs/1412.0439},
	doi = {10.1016/j.patcog.2014.11.016},
	abstract = {Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.},
	number = {5},
	urldate = {2018-08-24TZ},
	journal = {Pattern Recognition},
	author = {Lim, Chern Hong and Vats, Ekta and Chan, Chee Seng},
	month = may,
	year = {2015},
	note = {arXiv: 1412.0439},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1773--1796}
}

@article{vishwakarma_survey_2012,
	title = {A {Survey} on {Activity} {Recognition} and {Behavior} {Understanding} in {Video} {Surveillance}},
	volume = {29},
	doi = {10.1007/s00371-012-0752-6},
	abstract = {This paper provides a comprehensive survey for activity recognition in video surveillance. It starts with a description of simple and complex human activity, and various applications. The applications of activity recognition are manifold, ranging from visual surveillance through content based retrieval to human computer interaction. The organization of this paper covers all aspects of the general framework of human activity recognition. Then it summarizes and categorizes recent-published research progresses under a general framework. Finally, this paper also provides an overview of benchmark databases for activity recognition, the market analysis of video surveillance, and future directions to work on for this application.},
	journal = {The Visual Computer},
	author = {Vishwakarma, Sarvesh and Agrawal, Anupam},
	month = oct,
	year = {2012}
}

@article{zhang_rgb-d-based_2016,
	title = {{RGB}-{D}-based action recognition datasets: {A} survey},
	volume = {60},
	issn = {00313203},
	shorttitle = {{RGB}-{D}-based action recognition datasets},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320316301029},
	doi = {10.1016/j.patcog.2016.05.019},
	language = {en},
	urldate = {2018-08-24TZ},
	journal = {Pattern Recognition},
	author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip O. and Wang, Pichao and Tang, Chang},
	month = dec,
	year = {2016},
	pages = {86--105}
}

@article{chaquet_survey_2013-1,
	title = {A survey of video datasets for human action and activity recognition},
	volume = {117},
	issn = {10773142},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314213000295},
	doi = {10.1016/j.cviu.2013.01.013},
	language = {en},
	number = {6},
	urldate = {2018-08-24TZ},
	journal = {Computer Vision and Image Understanding},
	author = {Chaquet, Jose M. and Carmona, Enrique J. and Fernández-Caballero, Antonio},
	month = jun,
	year = {2013},
	pages = {633--659}
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2018-08-23TZ},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{hara_can_2017,
	title = {Can {Spatiotemporal} 3D {CNNs} {Retrace} the {History} of 2D {CNNs} and {ImageNet}?},
	url = {http://arxiv.org/abs/1711.09577},
	abstract = {The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4\% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5\% and 70.2\% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch},
	urldate = {2018-08-22TZ},
	journal = {arXiv:1711.09577 [cs]},
	author = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09577},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@inproceedings{cao_egocentric_2017,
	title = {Egocentric {Gesture} {Recognition} {Using} {Recurrent} 3D {Convolutional} {Neural} {Networks} with {Spatiotemporal} {Transformer} {Modules}},
	doi = {10.1109/ICCV.2017.406},
	abstract = {Gesture is a natural interface in interacting with wearable devices such as VR/AR helmet and glasses. The main challenge of gesture recognition in egocentric vision arises from the global camera motion caused by the spontaneous head movement of the device wearer. In this paper, we address the problem by a novel recurrent 3D convolutional neural network for end-to-end learning. We specially design a spatiotemporal transformer module with recurrent connections between neighboring time slices which can actively transform a 3D feature map into a canonical view in both spatial and temporal dimensions. To validate our method, we introduce a new dataset with sufficient size, variation and reality, which contains 83 gestures designed for interaction with wearable devices, and more than 24,000 RGB-D gesture samples from 50 subjects captured in 6 scenes. On this dataset, we show that the proposed network outperforms competing state-of-the-art algorithms. Moreover, our method can achieve state-of-the-art performance on the challenging GTEA egocentric action dataset.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Cao, C. and Zhang, Y. and Wu, Y. and Lu, H. and Cheng, J.},
	month = oct,
	year = {2017},
	keywords = {3D feature map, cameras, Cameras, egocentric gesture recognition, egocentric vision, end-to-end learning, feature extraction, gesture recognition, Gesture recognition, global camera motion, GTEA egocentric action dataset, image motion analysis, learning (artificial intelligence), natural interface, neighboring time slices, recurrent 3D convolutional neural network, recurrent 3D convolutional neural networks, recurrent connections, recurrent neural nets, RGB-D gesture samples, spatial dimensions, Spatiotemporal phenomena, spatiotemporal transformer module, spontaneous head movement, temporal dimensions, Three-dimensional displays, Two dimensional displays, Videos, virtual reality, VR-AR glasses, VR-AR helmet, wearable devices},
	pages = {3783--3791},
	file = {IEEE Xplore Abstract Record:/Users/A.Gunduz/Zotero/storage/MBES9JTQ/8237668.html:text/html}
}
