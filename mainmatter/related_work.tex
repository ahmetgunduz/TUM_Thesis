\chapter{Related Work}
\label{ch:relatedwork}

In recent years, with the development of powerful graphical processing units  (GPU), Deep neural networks (DNNs) has got deeper and deeper.  Relatedly, they grow even stronger by performing much better against conventional computer vision methods in many challenging tasks like gesture recognition.  The types of DNNs have also been increased exponentially  in the last decade.  Some of the most popular ones are Convolutional neural networks (CNNs), Recurrent neural networks (RNNs), and Generative adversarial networks (GANs).  Especially CNNs and its variants are very popular in computer vision community as a result of the fact that  CNN proves to work better than conventional computer vision methods in which machine learning methods build upon hand-crafted features when the dataset is large enough  \cite{krizhevsky2012imagenet}.\\

On the other hand, the number of available datasets in many areas increased drastically as a result of this success story.   In this \href{https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research}{link},  a list of popular open-source datasets can be found.   There are also many public datasets for action/gesture recognition tasks.   Some of these  datasets  are  Human  Motion  Data  Base (HMDB51)  \cite{kuehne_hmdb51:_2011}, Kinetics \cite{kay_kinetics_2017}, UCF 101 \cite{soomro_ucf101:_2012}, ActivityNet  \cite{heilbron_activitynet:_2015}, Jester  \cite{twentybn_20bn-jester_nodate}, Something- Something \cite{goyal_something_2017}, EgoGesture \cite{zhang_egogesture:_2018} and many more. Researchersâ€™ focus has been mainly on how to achieve the best accuracy by trying different models like in 2D tasks.  However, there is another question need to be answered here, and it is how to convert these models into a system works in real-time scenarios with the same accuracy as in offline classification.\\

With the availability of large datasets  (especially ImageNet),  CNNs has been used even more on 2D images where it can capture spatial information better than it ever had.  This changed the direction of researches to a whole new level.  New models were being trained first on ImageNet and then tuned on the dataset of interest.  Some example of this kind of applications can be face recognition, handwriting and character recognition,  and person recognition. The success of CNNs in object detection and classification tasks \cite{krizhevsky2012imagenet, zhou2014learning, Girshick2014rich} has created a growing trend to apply them also in other areas of computer vision.  Of course, the video action and activity recognition was one of the first application areas to apply  CNNs and achieve state-of-the-art performance \cite{simonyan2014two, Feichtenhofer2016convolutional, Feichtenhofer2016spatiotemporal}.\\

However, in most of these tasks (i.e., face recognition and object detection), spatial features are used as the essence of these tasks requires.  So in such cases, offline classification strategy is the same as real-time strategy,  which is running the model on every frame comes and make the prediction.Nevertheless,  this is not the case for dynamic action/gesture recognition tasks in which we need to be aware of not only spatial but also temporal relations since a continuous list of frames forms an action/gesture.  In the offline classification for these tasks,  it is common to use either  a fully-connected  or Recurrent neural networks (RNNs) in order to capture spatiotemporal patterns on top of CNN extracted features from a clip of frames. However, in online classification, we know neither when a gesture starts nor when it ends.\\

There have been various approaches using CNNs to extract spatiotemporal information from video data.   Due to the success of 2D CNNs in static images, first video analysis approaches also used 2D CNNs.  In \cite{Feichtenhofer2016convolutional, simonyan2014two, Karpathy2014largescale, Wang2015towards}, video frames are treated as multi-channel inputs to 2D CNNs.  Temporal  Segment Network  (TSN) \cite{wang2016temporal} divides the video into several segments, extracts information from color and optical flow modalities for each segment using 2D CNNs, and then applies spatiotemporal modeling for action recognition.  A convolutional long short-term memory (LSTM) architecture is proposed in \cite{donahue2015long}, where the authors extract first the features from video frames by a 2D CNN and then apply LSTM for global temporal modeling.  The strength of all these approaches comes from the fact that there are plenty of very successful 2D CNN architectures, and these architectures can be pretrained by ImageNet dataset \cite{deng2009imagenet}.\\

Although  2D CNNs perform pretty well on video analysis tasks, they are unable to model temporal information and motion patterns.  Therefore  3D CNNs have been proposed \cite{tran2015learning, tran2017convnet, hara3dcnns, varol2017long} which use 3D convolutions and 3D pooling to capture discriminative features along both spatial and temporal dimensions. Different from 2D CNNs, 3D CNNs take a sequence of video frames as inputs.  In this work, we also use the variants of 3D CNNs.\\

Fusion of information from different modalities is a widely applied technique in CNNs in order to increase the performance of the architectures.  Generally,  fusion techniques are categorized into three categories: data level \cite{kopuklu2018motion}, feature level \cite{Feichtenhofer2016convolutional, miao2017multimodal}  and decision level fusions \cite{simonyan2014two, molchanov2016online, zhou2017temporal}. Although the latter two are much more popular than data level fusion, they require separate training for each modality. A recently proposed data level fusion strategy  \cite{kopuklu2018motion}, Motion Fused Frames (MFFs), is used in this work. \\

Detecting dynamic hand gestures in real-time systems has various challenges.   First, these systems receive a continuous stream of video, where the gestures should be detected and classified simultaneously.   There are several works addressing detection and classification separately.   In \cite{ohn2014hand}, authors apply histogram of oriented gradient (HOG) algorithm together with an SVM classifier. A particular radar system is used to detect and segment gestures in \cite{molchanov2015multi}.  In our work, we have trained a lightweight 3D CNN for gesture detection.  Secondly, in human-computer interfaces, performed gestures must be recognized only once (i.e., single-time activations) by the computers.  This is very critical, and this problem has not been addressed well yet. 

Single-time activation per action/gesture is as crucial as classifying the action/gesture correctly.  For example, we want to do a hand gesture "a" which is making a circle in the counterclockwise direction, and do the detection in the way shown in Figure \ref{fig:old_workflow}.  On the other  hand,  moving the hand  to the right corresponds  to another gesture  "b"  and  moving the hand  to the  left corresponds  to gesture  "c". It  is expected  that  the  model will give activation  first for gesture  "b", then  for gesture "a", and at the ending gesture "c".  As a result of this, we approached the problem as a sequence to sequence learning  (Seq2Seq) task,  which requires one-time activation for each item in the sequence as well.\\

Seq2seq is about training models to convert sequences from one domain  (e.g., the audio signal for a word) to sequences in another domain (e.g., the same word in the form of text).  In Natural  Language Processing (NLP)  such as speech to text,  machine translation, and many other language-related tasks,  Seq2seq has proven its power in recent years. In this context,  video clips correspondence to the speech signal of a word and each gesture is a letter in that word.  For example, the input is an audio signal saying "hello", and the model for speech to text has created an output "h,e,l,l,o,o,o". It is obvious that the outcome need to have only one activation  for each letter  and the results should be "hello".  Similarly, in real-time gesture recognition, we have consecutive gestures performed.\\

In Seq2seq, it is handled using a decoder structure that finds the most probable path for an output sequence.  In Molchanov et al. \cite{molchanov_online_2016}, a Seq2seq approach is integrated from Natural  Language Processing (NLP)  into vision-based gesture recognition by applying connectionless temporal classification (CTC)  loss to detect consecutive similar gestures. However, this requires a different way of training procedure, and we intend to use already available models. Instead of going through a different training procedure,  in this work,  we achieved a similar result by using some popular post-processing methods on top of already trained models with conventional ways, which will be detailed in Section \ref{subsec:sta}. Moreover,  even though CTC loss approach is suitable for discriminating consecutive gestures and force the correct one stands out more clearly, it does not provide single-time activations. To the best of our knowledge, it is the first time that single-time activations are performed for deep learning based hand gesture recognition.